{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While for ordinary least squares regression we could directly compute parameters that yielded the best model, there is not such a closed-form solution anymore when we introduce a regularization term.\n",
    "In this exercise we take a look into the way that the ridge regression models, and actually most other regularized regression models are fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we use the Iris dataset to test our models on. The target variable is *petal width*, on we want to regress it from the three other numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dta = datasets.load_iris()\n",
    "#print(dta.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Exploring the Loss Function\n",
    "\n",
    "\n",
    "In lecture we have learned that in *ridge regression*, we want to minimize the loss function \n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2.\n",
    "$$\n",
    "\n",
    "#### a) Implementing the Loss\n",
    "\n",
    "Implement a function that computes the value of the loss function, using the signature in the cell below. Assume that the first column of your input matrix $X$ has all-constant values $1$ to model the intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 2-dimensional numpy array of features \n",
    "# y: 1-dimensional array of target values\n",
    "# beta: current parameter vector\n",
    "# reg: regularization term lambda (float)\n",
    "\n",
    "def loss(X,y,beta,reg):\n",
    "    #X = np.insert(X, 0, values=1, axis=1) # add a column of ones s.t. y=X*b\n",
    "    sse = np.sum(np.square(np.dot(X,beta) - y))\n",
    "    norm = np.sum(np.square(beta))\n",
    "    return sse + reg*norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Convexity of the Loss\n",
    "\n",
    "We explore the cost function with a practical example. Load the Iris dataset from sklearn, and set up a univariate regression in which you predict petal width from petal length.\n",
    "Set  $\\beta_0 = mean(y)$ and $\\lambda = 1$, and plot the value of the cost function against $\\beta_1$ for  $\\beta_1 \\in\\{-10,-9.9,\\dots,9.9,10\\}$.  \n",
    "Is there a unique minimum? What is the sign of the derivative at $\\beta = -3$ and $\\beta = 3$? In which direction does it point - in direction of the minimum or against it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([dta.data[:,2]]).T\n",
    "X = np.insert(X, 0, values=1, axis=1)\n",
    "\n",
    "y = dta.data[:,3]\n",
    "b0 = np.mean(y)\n",
    "losses = []\n",
    "iv = np.arange(-10,10, step = 0.1)\n",
    "for b1 in iv:\n",
    "    losses.append(loss(X,y,np.array([b0,b1]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAcYElEQVR4nO3df6yc1Z3f8fenJtBikmDia5bFZo2Nl0D+ATNy3KZdJWExNorWZMVWkBVYDa27CY6S7lYLaVZLFPpHoEqiUggpvxSDCJCSZLFSg+MSVlElIFwTMBDD+tphxQ0uvq4JASMlJXz7x3MGPx7P3Dt37sw8P+bzkkZ35jw/5swzz53vOed7nhlFBGZmZu38k6IrYGZm5eUgYWZmHTlImJlZRw4SZmbWkYOEmZl1dEzRFei3hQsXxtKlS4uuhplZpezYseNARIy1ltcuSCxdupTx8fGiq2FmVimS/rFduYebzMysIwcJMzPryEHCzMw6cpAwM7OOHCTMzKwjBwkzM+vIQcLMzDpykEgigon9b+KvTjczO8xBItkzdYhN33mKPVOHiq6KmVlpOEgky8fmc9OnVrJ8bH7RVTEzK43afS1HryRxxqITiq6GmVmpuCfRwrkJM7PDHCRaODdhZnaYg0QL5ybMzA6bMUhIWiLpUUm7JD0v6fOp/MuSfinp6XS7KLfNFyVNSHpR0oW58rWpbELSNbny0yU9IWm3pPslHZvKj0uPJ9Lypf188R1eL2csOgFJg34qM7PS66Yn8TbwVxFxFrAauErS2WnZNyLinHTbCpCWXQp8CFgLfFPSPEnzgJuBdcDZwGW5/Vyf9rUCeA24MpVfCbwWEWcA30jrDZzzEmZmmRmDRETsi4in0v03gF3AqdNssh64LyJ+ExG/ACaAVek2ERF7I+K3wH3AemVN9o8DD6TtNwMX5/a1Od1/ADhfQ2jiOy9hZpaZVU4iDfecCzyRijZJ2inpTkkLUtmpwMu5zSZTWafyDwC/ioi3W8qP2Fda/npaf6CclzCzqhnUCEjXQULSCcD3gC9ExK+BW4DlwDnAPuBrzVXbbB49lE+3r9a6bZQ0Lml8ampq2tfRDeclzKxqBjUC0lWQkPQesgBxT0R8HyAiXo2I30XEO8BtZMNJkPUEluQ2Xwy8Mk35AeBESce0lB+xr7T8/cDB1vpFxK0R0YiIxtjYUb/j3TPnJsysKgY1AtLN7CYBdwC7IuLrufJTcqt9Engu3d8CXJpmJp0OrAB+CjwJrEgzmY4lS25viewT+FHgkrT9BuDB3L42pPuXAD+OIX5iOzdhZlUxqBGQbr6W4yPA5cCzkp5OZf+JbHbSOWTDPy8B/x4gIp6X9F3g52Qzo66KiN+lF7EJ2AbMA+6MiOfT/q4G7pP0n4GfkQUl0t+7JU2Q9SAuncNrnTXnJsys7CKCPVOHWD42fyBD5KrbUEqj0Yjx8fGiq2FmNhQT+99k03ee4qZPrZzT989J2hERjdZyX3E9A+clzKzMBj3i4SAxA+clzKyMmg1YYKCzMR0kZuC8hJmV0bAasA4SM8j/zoSHncysLIbVgHWQ6JKHncysTIZ10a+DRJc87GRmZTHMCTUOEl3yV3WYWVkMc2TDQWIWPB3WzMpgmCMbDhKz4LyEmZXBMEc2HCRmwXkJMytSEaMZDhKz4LyEmRWpiNEMB4keODdhZkUoYjTDQaIHzk2YWRGKGM1wkOiBcxNmNmxFjWA4SPRAEsvH5rNn6pCHnMxsKIoawXCQ6JGHnMxsmIoawXCQ6JGHnMxsGIb1leCdOEj0yNNhzWwYih61cJCYI0+HNbNBKnrUwkFijoqO8mZWb0WPWjhIzFHRUd7M6qsMIxUOEnNUdJQ3s/oqw0iFg0QflCHam1n9lGGkwkGiD8oQ7c2sPoqe9prnINEHZYj2ZlYfZWp4Okj0QTMvAXjYyczmrEwNTweJPipT9Dez6irThBgHiT4qU/Q3s2oq20QYB4k+KlP0N7NqKtuIxIxBQtISSY9K2iXpeUmfT+UnSdouaXf6uyCVS9KNkiYk7ZS0MrevDWn93ZI25MrPk/Rs2uZGpU/ZTs9RdmVrCZhZdZRtRKKbnsTbwF9FxFnAauAqSWcD1wCPRMQK4JH0GGAdsCLdNgK3QPaBD1wLfBhYBVyb+9C/Ja3b3G5tKu/0HKVWtpaAmZVfmaa95s0YJCJiX0Q8le6/AewCTgXWA5vTapuBi9P99cBdkXkcOFHSKcCFwPaIOBgRrwHbgbVp2fsi4rHImt53teyr3XOUWtlaAmZWfmVtXM4qJyFpKXAu8ARwckTsgyyQAIvSaqcCL+c2m0xl05VPtilnmudorddGSeOSxqempmbzkgbCv1xnZrNV1sZl10FC0gnA94AvRMSvp1u1TVn0UN61iLg1IhoR0RgbG5vNpgNT1laBmZVTWSe+dBUkJL2HLEDcExHfT8WvpqEi0t/9qXwSWJLbfDHwygzli9uUT/ccpVfWVoGZlU+ZJ7t0M7tJwB3Aroj4em7RFqA5Q2kD8GCu/Io0y2k18HoaKtoGrJG0ICWs1wDb0rI3JK1Oz3VFy77aPUfp+SpsM+tWmUceuulJfAS4HPi4pKfT7SLgq8AFknYDF6THAFuBvcAEcBvwWYCIOAhcBzyZbl9JZQCfAW5P2+wBHkrlnZ6jMsr85ptZOZR55EF1a+E2Go0YHx8vuhrvigj2TB1i+dj80o01mlmxyvT5IGlHRDRay33F9YCVNRllZsWrwkiDg8QQlDkpZWbFKfMwU5ODxBBUobVgZsNVpqGm6ThIDEEVWgtmNlxVaTw6SAyBp8OaWauqNB4dJIaoKi0HMxu8qkxqcZAYoqq0HMxscKo2kcVBYoj8xX9mVrURBQeJIavaCWJm/VW1EQUHiSGr2gliZv1TlWmveQ4SQ1aVZJWZ9V8VRxIcJApSteSVmc1dFUcSHCQKUsUWhZn1pqy/X90NB4mCVLFFYWa9qXKj0EGiIJ4OazY6qtwodJAoUJVbF2bWnSrOaMpzkChQlVsXZtadqjcGHSQK5C/+M6u/qjcGHSRKoOotDTM7WpVnNOU5SJRA1VsaZna0ujT+HCRKwDOdzOqnLo0/B4mSqEurw8wydfkKHgeJkqhLq8Ns1NXtK3ccJErCM53M6qFuowIOEiVTtxPMbNTUbVTAQaJk6naCmY2Sql9d3Y6DRMl4ppNZddVxJMBBooTqeKKZjYI6jgQ4SJRQHU80szqry9XV7cwYJCTdKWm/pOdyZV+W9EtJT6fbRbllX5Q0IelFSRfmytemsglJ1+TKT5f0hKTdku6XdGwqPy49nkjLl/brRZedZzqZVUude//d9CS+DaxtU/6NiDgn3bYCSDobuBT4UNrmm5LmSZoH3AysA84GLkvrAlyf9rUCeA24MpVfCbwWEWcA30jrjZQ6n3hmdVLn3v+MQSIifgIc7HJ/64H7IuI3EfELYAJYlW4TEbE3In4L3AesV9Yn+zjwQNp+M3Bxbl+b0/0HgPNVpz5cF+p84pnVRR1nNOXNJSexSdLONBy1IJWdCrycW2cylXUq/wDwq4h4u6X8iH2l5a+n9Y8iaaOkcUnjU1NTc3hJ5eKZTmblV/cef69B4hZgOXAOsA/4WipvF0ajh/Lp9nV0YcStEdGIiMbY2Nh09a6cup+AZlVX9x5/T0EiIl6NiN9FxDvAbWTDSZD1BJbkVl0MvDJN+QHgREnHtJQfsa+0/P10P+xVG3U/Ac2qqs4zmvJ6ChKSTsk9/CTQnPm0Bbg0zUw6HVgB/BR4EliRZjIdS5bc3hLZGMqjwCVp+w3Ag7l9bUj3LwF+HCM45uKZTmblNCq9/G6mwN4LPAacKWlS0pXADZKelbQT+BjwHwAi4nngu8DPgYeBq1KP421gE7AN2AV8N60LcDXwl5ImyHIOd6TyO4APpPK/BN6dNjuKRuWENKuKUenlq24t00ajEePj40VXo+/qPoPCrErq+P8oaUdENFrLfcV1RXimk1l5jFLP3kGiQkbpxDQro2ayetnC40diqAkcJCplVMZAzcqq2VDbe+CtWs9oynOQqBDPdDIr1ig21BwkKsjDTmbDV8dkdTccJCpoFFszZkUb1caZg0QFedjJbPhGtXHmIFFho9qyMRumUfn6jU4cJCps+dh8/ttl50KEexNmAzLqjTEHiQqThCQ23fuzkT2BzQZtVIeZmhwkKm7UT2CzQRrVGU15DhIV5yS22eCM+lATOEjUhk9ms/5zT91BojZ8Mpv1z6jPaMpzkKgJf0usWf+4Z36Yg0SN+MQ2m7tIU8pvuuxc98xxkKgVDzmZzd2eqUN87t6fQZpiPuocJGrEM53M5s6NrSM5SNSQh53MZs/J6vYcJGrIX9dhNntuXLXnIFFD/roOs9lxsrozB4ma8riqWfecrO7MQaKmnMQ2654bVZ05SNScx1nNOnOyemYOEjXnJLZZZ25EzcxBouacxDbrzMNMM3OQGAH+RzA7mn8rojsOEiPASWyzo3moqTszBglJd0raL+m5XNlJkrZL2p3+LkjlknSjpAlJOyWtzG2zIa2/W9KGXPl5kp5N29yoFNI7PYf1zv8UZoeT1csWHu8edhe66Ul8G1jbUnYN8EhErAAeSY8B1gEr0m0jcAtkH/jAtcCHgVXAtbkP/VvSus3t1s7wHNYjJ7HNDjeW9h54yzOaujBjkIiInwAHW4rXA5vT/c3AxbnyuyLzOHCipFOAC4HtEXEwIl4DtgNr07L3RcRjkX1q3dWyr3bPYT1yEttGmXsQvek1J3FyROwDSH8XpfJTgZdz602msunKJ9uUT/ccNgdOYtuocg+iN/1OXLc76tFD+eyeVNooaVzS+NTU1Gw3HylOYtuocgOpN70GiVfTUBHp7/5UPgksya23GHhlhvLFbcqne46jRMStEdGIiMbY2FiPL2m0OIlto8TTXXvXa5DYAjRnKG0AHsyVX5FmOa0GXk9DRduANZIWpIT1GmBbWvaGpNVpVtMVLftq9xzWB05i2yhxo6h33UyBvRd4DDhT0qSkK4GvAhdI2g1ckB4DbAX2AhPAbcBnASLiIHAd8GS6fSWVAXwGuD1tswd4KJV3eg7rAyexbRQ4WT13qlsrstFoxPj4eNHVqIRmF3zZwuPZe+Atd8Wtdib2v8mm7zzFTZ9a+W4uztqTtCMiGq3lvuJ6hDWT2HsPvOWuuNWSk9Vz5yBh/keyWnKyuj8cJAxJLB+bz56pQ05iW204Wd0fDhIG+B/K6sPJ6v5ykDDg8JDTsoXH+yI7qzRfWd1fDhIGOIlt9RDpup+bLjvXPYg+cZCwI/giO6uyPVOH+Ny9P4N0HZDNnYOEHcEX2VkVOQ8xOA4SdhTnJ6xqnIcYHAcJO4rzE1YlzkMMloOEdeSL7KwKnIcYLAcJ68i/PWFV4MbMYDlI2Ix8oZ2VUTNZDTgPMUAOEjYjT4u1MnLjZTgcJGxGnhZrZeNk9fA4SFhXPC3WysTJ6uFxkLCueFqslYEvmhs+BwmbFecnrEi+aG74HCRsVpyfsKI4D1EMBwmbNecnrAjOQxTDQcJmzfkJGybnIYrlIGE9c37ChsF5iGI5SFjPnJ+wQXMeongOEjYnzk/YIDkPUTwHCZsT5ydsEJyHKA8HCesL5yesXyKCR1+cch6iJBwkrC/y+YmJ/W966Ml6tmfqENc/tIu/XvtB9yBKwEHC+qaZnxB46MlmLT/EdPOfn8fHzhxzD6IEHCSsb5r5ieWLTvDQk82ap7qW05yChKSXJD0r6WlJ46nsJEnbJe1Ofxekckm6UdKEpJ2SVub2syGtv1vShlz5eWn/E2lbnzUV4KmxNhtOUpdbP3oSH4uIcyKikR5fAzwSESuAR9JjgHXAinTbCNwCWVABrgU+DKwCrm0GlrTOxtx2a/tQXxsCT421brkHUW6DGG5aD2xO9zcDF+fK74rM48CJkk4BLgS2R8TBiHgN2A6sTcveFxGPRfYJc1duX1Zynhpr3fDFcuU31yARwI8k7ZC0MZWdHBH7ANLfRan8VODl3LaTqWy68sk25UeRtFHSuKTxqampOb4k6ydPjbV2mkNMe/a/6YvlSm6uQeIjEbGSbCjpKkl/NM267c6A6KH86MKIWyOiERGNsbGxmepsQ+SpsdZOc4gpwHmIkptTkIiIV9Lf/cAPyHIKr6ahItLf/Wn1SWBJbvPFwCszlC9uU24V46mxlpcfYjpj0QnOQ5Rcz0FC0nxJ723eB9YAzwFbgOYMpQ3Ag+n+FuCKNMtpNfB6Go7aBqyRtCAlrNcA29KyNyStTrOarsjtyyrEU2Mtz9/HVC1z6UmcDPxvSc8APwX+Z0Q8DHwVuEDSbuCC9BhgK7AXmABuAz4LEBEHgeuAJ9PtK6kM4DPA7WmbPcBDc6ivFcxTY0ebp7pWk+rWoms0GjE+Pl50NayDiGDP1CGWLTyevQfeYvnYfLcmR0Dz+5huePgFbvrUSs5YdELRVbIWknbkLmV4l6+4tqHKT4296p4dPPrilIeeRoC/j6m6HCSsEMvH5nP1urO44eEXPPRUY/4+pupzkLBCSOJjZ475quwa81d+14ODhBWm9apsX0dRLx5iqgcHCSucr6OoFw8x1YuDhBXO11HUh4eY6sdBwkrDX+FRfR5iqh8HCSuV/NCTp8hWh4eY6stBwkolP/TkKbLV4CGmejum6AqYtdOcInvaSce/O0XWV2eXS/PqeSK4/qFdXL3uLA8x1ZB7ElZavjq73PJf9+0hpvpyT8JKL3919pIF/wxJ7lUU7IhflPPwUq25J2Gll78629dSFMu/KDd6HCSsEnwtRfHyCWr/otzocJCwSvG1FMOX7z00r4HwL8qNDgcJqxxfSzEc+eDgBPXocpCwyml3LYV7Ff3VbmjJvYfR5NlNVln5aymI4Kp7dnD1urPc0p2Ddtc+ODCMNvckrNLcq+if1t6Dh5YM3JOwmnCvonfuPdh03JOw2nCvYvbce7CZuCdhteNexczce7BuuSdhteReRWfuPdhsuCdhtdbaq9j0naeyK7YBwUh975B7D9YL9ySs9vK9iuZFeBvvHmfj3Ttq37toXhD3zjvvuPdgPXFPwkZGM1hEBLde3qD58djMWXz0Dxey98BbtfmG2eaw0g0Pv8Bfr/2gew/WEwcJGzmSWHHye4Hsg7SZs4DDH6RVbWU3ew7Nmjdfz0f/cCGnnXRebQKgDY+DhI201l/Ag8O/WwHVyFu0BoaNd48jxH+//Dxu/vPDgeGMRScUWk+rJtVtLLbRaMT4+HjR1bCKyid3/13uwxbKEzCadVy28Hj2Hnir1HW16pC0IyIaR5WXPUhIWgv8V2AecHtEfHW69R0krB9aW+ftPoSXjc0fWg4jHxj+/h8OvJtnuOHhF0Z2tpb1V6cgUerhJknzgJuBC4BJ4ElJWyLi58XWzOquNW+RT3Q3A8bffOLsI8b890wdmnPwyAeD/P7ygeHIPMNK5xlsoEodJIBVwERE7AWQdB+wHnCQsKHpFDCWjc2nmcOAD3LdD59/N3i0tvCXjc0/4kO/0/18MMjvr1MC2nkGG7SyB4lTgZdzjyeBD7euJGkjsBHgtNNOG07NbCTlAwZwRNJ7SS54nHbSyiNyBX/zibOP+NDvdD8fDI7cnwODFaPUOQlJfwZcGBH/Nj2+HFgVEZ/rtI1zElYW+bxGtz2JOl2nYdVSyZwEWc9hSe7xYuCVgupiNiutvY5u7ruXYGVT9q/leBJYIel0SccClwJbCq6TmdnIKHVPIiLelrQJ2EY2BfbOiHi+4GqZmY2MUgcJgIjYCmwtuh5mZqOo7MNNZmZWIAcJMzPryEHCzMw6cpAwM7OOSn0xXS8kTQH/2OPmC4EDfaxOv5S1XlDeurles1PWekF561a3ev1BRIy1FtYuSMyFpPF2VxwWraz1gvLWzfWanbLWC8pbt1Gpl4ebzMysIwcJMzPryEHiSLcWXYEOylovKG/dXK/ZKWu9oLx1G4l6OSdhZmYduSdhZmYdOUiYmVlHIxckJP2ZpOclvSOp0bLsi5ImJL0o6cIO258u6QlJuyXdn77CvN91vF/S0+n2kqSnO6z3kqRn03pD+aUlSV+W9Mtc/S7qsN7adBwnJF0zhHr9F0kvSNop6QeSTuyw3lCO2UyvX9Jx6X2eSOfT0kHVJfecSyQ9KmlX+h/4fJt1Pirp9dz7+7eDrld63mnfF2VuTMdrp6SVQ6rXmblj8bSkX0v6Qss6Qzlmku6UtF/Sc7mykyRtT59H2yUt6LDthrTObkkbZvXEETFSN+As4Ezg74FGrvxs4BngOOB0YA8wr8323wUuTfe/BXxmwPX9GvC3HZa9BCwc8vH7MvAfZ1hnXjp+y4Bj03E9e8D1WgMck+5fD1xf1DHr5vUDnwW+le5fCtw/hPfuFGBluv9e4B/a1OujwA+HeU51874AFwEPkf1k+GrgiQLqOA/4P2QXnQ39mAF/BKwEnsuV3QBck+5f0+68B04C9qa/C9L9Bd0+78j1JCJiV0S82GbReuC+iPhNRPwCmABW5VdQ9puSHwceSEWbgYsHVdf0fP8auHdQzzEgq4CJiNgbEb8F7iM7vgMTET+KiLfTw8fJfsWwKN28/vVk5w9k59P5GvBvlkbEvoh4Kt1/A9hF9jvyVbAeuCsyjwMnSjplyHU4H9gTEb1+o8OcRMRPgIMtxfnzqNPn0YXA9og4GBGvAduBtd0+78gFiWmcCrycezzJ0f9AHwB+lfswardOP/0r4NWI2N1heQA/krRD0sYB1qPVptTlv7ND97abYzlInyZrdbYzjGPWzet/d510Pr1Odn4NRRreOhd4os3ify7pGUkPSfrQkKo00/tS9DkFWY+vU4OtiGMGcHJE7IOsEQAsarPOnI5d6X90qBeS/hfwe20WfSkiHuy0WZuy1vnB3azTlS7reBnT9yI+EhGvSFoEbJf0QmptzMl0dQNuAa4je93XkQ2Hfbp1F222nfNc626OmaQvAW8D93TYzUCOWWtV25QN7FyaLUknAN8DvhARv25Z/BTZcMqbKd/0d8CKIVRrpvelsOMFkHKPfwJ8sc3ioo5Zt+Z07GoZJCLij3vYbBJYknu8GHilZZ0DZN3cY1Lrr906famjpGOAPwXOm2Yfr6S/+yX9gGyYY84feN0eP0m3AT9ss6ibY9n3eqWE3CeA8yMNxrbZx0COWYtuXn9zncn0Xr+fo4cS+k7Se8gCxD0R8f3W5fmgERFbJX1T0sKIGOgX2XXxvgzknJqFdcBTEfFq64KijlnyqqRTImJfGn7b32adSbK8SdNispxsVzzcdNgW4NI06+R0spbAT/MrpA+eR4FLUtEGoFPPZK7+GHghIibbLZQ0X9J7m/fJErfPtVu3n1rGgT/Z4TmfBFYomwl2LFk3fcuA67UWuBr4k4h4q8M6wzpm3bz+LWTnD2Tn0487BbZ+STmPO4BdEfH1Duv8XjM3ImkV2WfE/x1wvbp5X7YAV6RZTquB15vDLEPSsVdfxDHLyZ9HnT6PtgFrJC1Iw8NrUll3Bp2RL9uN7INtEvgN8CqwLbfsS2SzUl4E1uXKtwK/n+4vIwseE8D/AI4bUD2/DfxFS9nvA1tz9Xgm3Z4nG3IZxvG7G3gW2JlO0FNa65YeX0Q2e2bPMOqW3o+XgafT7Vut9RrmMWv3+oGvkAUxgH+azp+JdD4tG8Ix+pdkwww7c8fpIuAvmucasCkdm2fIJgD8iyHUq+370lIvATen4/ksuZmJQ6jf8WQf+u/PlQ39mJEFqX3A/0ufYVeS5bEeAXanvyeldRvA7bltP53OtQng38zmef21HGZm1pGHm8zMrCMHCTMz68hBwszMOnKQMDOzjhwkzMysIwcJMzPryEHCzMw6+v98IvfMKTODkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(iv,losses,s=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ We observe that this loss function is convex (i.e., if we connect any two points in the plot with a line, this line would be above the graph), and thus there is a unique minimum. Note that this would also hold if we were to 3D-plot the loss with respect to $\\beta_0$ and $\\beta_1$, and also for higher-dimensional vectors $\\beta$.  \n",
    "At $\\beta=-3$, the loss funtion appears to be falling, and thus the slope is negative, i.e. the derivative has negative sign. However, the minimum is in positive direction, and thus it points in the opposite direction of the minimum. Thus, if we wanted to wander toward the minimum from $\\beta=-3$, we would have to wander against the direction of the gradient.  \n",
    "The analogous case holds for $\\beta=3$, only with opposite signs/direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Introducing Gradient Descent\n",
    "\n",
    "In practice, regularized regression models regression is usually optimized by a variant of gradient descent, which is a conpetually simple iterative procedure, in which you first initialize $\\beta$ at random, and then, until convergence, one updates\n",
    "$$\\beta^{new} \\gets \\beta^{old} - \\alpha\\cdot \\nabla L(\\beta^{old}), $$\n",
    "where $\\alpha$ is the so-called learning rate, and $\\nabla L(\\beta)$ the gradient of the loss function with respect to $\\beta$.\n",
    "In practice one usually selects a convergence tolerance $\\epsilon>0$ and stops the iteration when $|L(\\beta^{new}) - L(\\beta^{old})| < \\epsilon$.\n",
    "\n",
    "#### a) Computing the gradient\n",
    "Grab a pen and a piece of paper and compute the partial derivatives $$\\frac{\\partial C(\\beta)}{\\partial \\beta_k}$$ for $k=0$ and $k>0$. Judging the resulting derivatives, give a reason why it is not ideal when the features are not standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n -2x_{ik} \\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right) + 2\\alpha\\beta_k\n",
    "$$\n",
    "For $\\beta_0$, we get the same result, only that we would have $x_{ik}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implementing Gradient Descent for Ridge Regression\n",
    "Implement a function that optimizes a ridge regression model via gradient descent, using the function signature in the cell below. Initialize your model in a way that $\\beta_0$ equals the mean of $y$ and all other elements of $\\beta$ are drawn from the standard normal distribution.\n",
    "\n",
    "Test your model on the **scaled** iris data to predict the petal width from all other columns, using $\\lambda = 1$, $\\alpha=0.001$, $\\epsilon=10^{-6}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES\n",
    "# X: 2-dimensional numpy array of features \n",
    "# y: 1-dimensional array of target values\n",
    "# reg: regularization term lambda (float)\n",
    "# alpha: learning rate (float)\n",
    "# eps: convergence treshold (float)\n",
    "# max_iter: maximum number of iterations in gradient descent (int) \n",
    "# -> break iteration when that number is reached even is we have not yet converged\n",
    "#\n",
    "# OUTPUT values:\n",
    "#\n",
    "# beta: numpy array containing optimal coefficients\n",
    "# n_it: number of iterations\n",
    "# losses: vector of loss function values in every iteration\n",
    "\n",
    "def ridge_GD(X,y,reg,alpha,eps,max_iter=10000): \n",
    "    n = (np.shape(X)[0])\n",
    "    p = np.shape(X)[1] # p is the number of variables in the multiple regression\n",
    "    \n",
    "    # initilize beta vector\n",
    "    b = np.random.normal(size=p+1)\n",
    "    b[0] = np.mean(y)\n",
    "    X = np.insert(X, 0, values=1, axis=1) # add a column of ones s.t. y=X*b\n",
    "\n",
    "    # initialize losses\n",
    "    losses = [loss(X,y,b,reg)]\n",
    "    n_it = 0\n",
    "    diff = eps+1\n",
    "    \n",
    "    # now the gradient descent:\n",
    "    while n_it < max_iter and diff > eps:\n",
    "        \n",
    "        b_old = np.copy(b)\n",
    "        res = (y - np.dot(X,b)) # residual from prediction with current beta\n",
    "        \n",
    "        # gradient computation can be written in a compact way\n",
    "        d_b = (-1)*np.dot(X.T,(2*res))+2*reg*b # derivative w.r.t. b\n",
    "        \n",
    "        # parameter update\n",
    "        b = b_old - alpha*d_b # b = b -1/n * learning_rate*gradient(f)\n",
    "        losses.append(loss(X,y,b,reg))\n",
    "        diff = np.linalg.norm(losses[-1]-losses[-2])\n",
    "        \n",
    "        n_it+=1\n",
    "      \n",
    "    if n_it >=max_iter:\n",
    "        print(\"Iteration has not yet converged\")\n",
    "        \n",
    "    return b, n_it, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.19139073 -0.1272431   0.07930119  0.87050427]\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = dta.data[:,:-1]\n",
    "y = dta.data[:,-1]\n",
    "scaler = StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "beta,n_it,losses = ridge_GD(X,y,reg=1,alpha=1e-03,eps=1e-06)\n",
    "print(beta)\n",
    "print(n_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.199333333333335, array([-0.12810076,  0.0796349 ,  0.87146313]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# our solution roughly cooresponds to sklearn's ridge regression\n",
    "rr = linear_model.Ridge(alpha=1)\n",
    "rr = rr.fit(X,y)\n",
    "print((rr.intercept_,rr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) The Effect of the Learning Rate\n",
    "\n",
    "Reoptimize your model using different learning rates $\\alpha \\in \\{10^{-i} : i\\in\\{1,2,3,4,5,6\\}\\}$.\n",
    "What are the effects on your optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in square\n",
      "  \n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in square\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 88, array([ 1.22789174e+141, -4.95088303e+155,  2.84228169e+155,\n",
      "       -5.41760005e+155]), inf)\n",
      "(0.01, 218, array([ 1.22897555e+139, -6.56186036e+153,  3.76713719e+153,\n",
      "       -7.18044333e+153]), inf)\n",
      "(0.001, 305, array([ 1.19139073, -0.12724653,  0.07930252,  0.8705081 ]), 7.64236731036566)\n",
      "(0.0001, 2160, array([ 1.19139073, -0.12534369,  0.07856213,  0.86838075]), 7.642559883046041)\n",
      "Iteration has not yet converged\n",
      "(1e-05, 10000, array([1.19139073, 0.00911081, 0.02624584, 0.71806213]), 8.169954231387898)\n",
      "Iteration has not yet converged\n",
      "(1e-06, 10000, array([ 1.19177816, -1.18132321,  0.49142224,  2.04939312]), 38.76020716590014)\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1e-01,1e-02,1e-03,1e-04,1e-05,1e-06]:\n",
    "    beta,n_it,losses = ridge_GD(X,y,reg=1,alpha=alpha,eps=1e-06)\n",
    "    print((alpha,n_it,beta,losses[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ If the learning rate is too big, we actually keep leaping over the minimum, in a way that with each update, we even end up further away from the minimum. Contrarily, of the learning rate is too low, we just take much longer to converge to the minimum.  \n",
    "Note that for these reason, in practice one applies dynamic learning rates, which however are beyond the scope of this lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Stochastic Gradient Descent \n",
    "\n",
    "Note that in practice, the datasets that are optimized on are often very large. When optimizing the model, this has the particular effect that the sum becomes very large and thus takes very long to be computed. one often uses more advanced techniques such as stochastic gradient descent (SGD) to accelerate the iteration.\n",
    "The idea behind SGD is to only consider a subset (batch) of samples which is randomly drawn from the full data. That way, we only have to sum over the small subset of samples, which significantly accelerates each iteration step. While this optimizes the cost function in a rather noisy way, and usually takes more single iteration steps, this approach still saves much computation time on large datasets.\n",
    "\n",
    "#### a) Implementing SGD\n",
    "Adapt your code from task 2 to implement stochastic gradient descent! Apply your model on the scaled iris data with batch size 10, and all other parameters as in 2a)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES\n",
    "# X: 2-dimensional numpy array of features \n",
    "# y: 1-dimensional array of target values\n",
    "# reg: regularization term lambda (float)\n",
    "# batch_size: (int)\n",
    "# alpha: learning rate (float)\n",
    "# eps: convergence treshold (float)\n",
    "# max_iter: maximum number of iterations in gradient descent (int) \n",
    "# -> break iteration when that number is reached even is we have not yet converged\n",
    "#\n",
    "# OUTPUT values:\n",
    "#\n",
    "# beta: numpy array containing optimal coefficients\n",
    "# n_it: number of iterations\n",
    "# losses: vector of loss function values in every iteration\n",
    "\n",
    "def ridge_SGD(X,y,reg,batch_size,alpha,eps,max_iter=1e05):\n",
    "    \n",
    "    n = (np.shape(X)[0])\n",
    "    p = np.shape(X)[1] # p is the number of variables in the multiple regression\n",
    "    b = np.random.normal(size=p+1)\n",
    "    b[0] = np.mean(y)\n",
    "    X = np.insert(X, 0, values=1, axis=1) # add a column of ones s.t. y=X*b\n",
    "\n",
    "    losses = [loss(X,y,b,reg)]\n",
    "    n_it = 0\n",
    "    diff = eps+1\n",
    "    \n",
    "    # now the gradient descent:\n",
    "    while n_it < max_iter and diff > eps:\n",
    "        \n",
    "        # draw indices of batch here\n",
    "        batch_ind = np.random.choice(n, size=batch_size, replace=False, p=None)\n",
    "        \n",
    "        b_old = np.copy(b)\n",
    "        \n",
    "        # onl consider samples in current batch\n",
    "        res = (y[batch_ind] - np.dot(X[batch_ind],b))\n",
    "        \n",
    "        # derivative w.r.t. b on current batch\n",
    "        d_b = (-1)*np.dot(X[batch_ind].T,(2*res))+2*reg*b \n",
    "        \n",
    "        # now parameter upodate as before\n",
    "        b = b_old - alpha*d_b # b = b -1/n * learning_rate*gradient(f)\n",
    "\n",
    "        losses.append(loss(X,y,b,reg))\n",
    "        diff = np.linalg.norm(losses[-1]-losses[-2])\n",
    "\n",
    "        n_it+=1\n",
    "      \n",
    "    if n_it >=max_iter:\n",
    "        print(\"Iteration has not yet converged\")\n",
    "        \n",
    "    return b, n_it, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Experimenting with Batch Sizes\n",
    "Reoptimize your model multiple times with fixed batch sizes in $\\{1,5, 10,20,30,50\\}$. What effects do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32648, array([ 0.59578457,  0.18311179, -0.0666366 ,  0.26463681]), 75.47578214786498)\n",
      "Iteration has not yet converged\n",
      "(1, 100000, array([ 0.59556358,  0.18309987, -0.07345723,  0.2522746 ]), 76.40666361896335)\n",
      "(1, 30865, array([ 0.59734585,  0.1989054 , -0.07664913,  0.27808831]), 73.06712328014015)\n",
      "(1, 81610, array([ 0.59809556,  0.20313632, -0.0635151 ,  0.2692286 ]), 73.6434033813567)\n",
      "(1, 56969, array([ 0.59853453,  0.1742241 , -0.04715589,  0.26294289]), 76.18131151173985)\n",
      "(5, 14229, array([ 1.0101501 ,  0.17932525, -0.05405334,  0.47210221]), 16.13386796673647)\n",
      "(5, 88684, array([ 0.9959433 ,  0.17519356, -0.05223313,  0.46190852]), 17.237735732558104)\n",
      "Iteration has not yet converged\n",
      "(5, 100000, array([ 0.99636925,  0.18274903, -0.04297986,  0.47034655]), 16.955282350701076)\n",
      "Iteration has not yet converged\n",
      "(5, 100000, array([ 0.99665048,  0.17803087, -0.0477636 ,  0.46141614]), 17.191957577599183)\n",
      "(5, 17447, array([ 1.01038767,  0.17584984, -0.05642789,  0.46443762]), 16.336882845616667)\n",
      "(10, 30475, array([ 1.09165847,  0.11135942, -0.01084483,  0.56166969]), 11.291322371461433)\n",
      "(10, 64388, array([ 1.08600265,  0.11059063, -0.02178412,  0.57060044]), 11.294738076511097)\n",
      "Iteration has not yet converged\n",
      "(10, 100000, array([ 1.08409443,  0.10533749, -0.02701072,  0.56639138]), 11.451640408930691)\n",
      "(10, 76957, array([ 1.09412001,  0.12159223, -0.02020898,  0.56871615]), 11.05505518098786)\n",
      "Iteration has not yet converged\n",
      "(10, 100000, array([ 1.09697486,  0.10915699, -0.02720403,  0.57281718]), 10.94186414770333)\n",
      "(20, 54305, array([1.14751976, 0.02590609, 0.01196684, 0.67974507]), 8.73426447818157)\n",
      "(20, 46489, array([1.15712623, 0.03543979, 0.01229716, 0.67585669]), 8.64975753659336)\n",
      "(20, 27054, array([1.14684418, 0.03091494, 0.01189032, 0.67829604]), 8.752293867929545)\n",
      "(20, 10104, array([1.1377864 , 0.04403393, 0.007398  , 0.66487298]), 9.003182710062534)\n",
      "(20, 5723, array([1.13304296, 0.03068822, 0.0090972 , 0.66877123]), 9.058819321857385)\n",
      "(30, 57693, array([ 1.15793072, -0.00443718,  0.03392125,  0.72673336]), 8.267363763601033)\n",
      "(30, 5155, array([ 1.17138995, -0.02060413,  0.04533336,  0.75182601]), 8.025784102558108)\n",
      "(30, 86337, array([ 1.15390693, -0.00999907,  0.03807005,  0.72986747]), 8.292236987914348)\n",
      "(30, 63369, array([ 1.16435331, -0.01323232,  0.02938141,  0.73444337]), 8.163756755899632)\n",
      "(30, 10987, array([ 1.14840911, -0.01705254,  0.03707306,  0.72796007]), 8.380540817274857)\n",
      "(50, 73424, array([ 1.175318  , -0.06349469,  0.04944794,  0.79361575]), 7.815729563281563)\n",
      "(50, 9496, array([ 1.18535496, -0.0682079 ,  0.06219692,  0.80545223]), 7.748317681074689)\n",
      "(50, 26553, array([ 1.174987  , -0.06283975,  0.04938446,  0.7936019 ]), 7.817685070697386)\n",
      "(50, 8410, array([ 1.17048013, -0.05993681,  0.05275372,  0.78485876]), 7.873162728035247)\n",
      "(50, 2488, array([ 1.18153346, -0.06313597,  0.05007746,  0.79451898]), 7.788333462873595)\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [1,5,10,20,30,50]:\n",
    "    for _ in range(5):\n",
    "        beta,n_it,losses = ridge_SGD(X,y,reg=1,batch_size=batch_size,alpha=1e-03,eps=1e-06)\n",
    "        print((batch_size,n_it,beta,losses[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ We generally need more iterations, though the exact number also strongly depends on the drawn batches. Further, the small batch sizes may result in small gradients, with a chance of braking the iteration too early due to a small update.  \n",
    "Yet, in practice SGD is still often preferred over standard GD, however when having small datasets like in our case, it is not necessarily the best choice as we can see here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
