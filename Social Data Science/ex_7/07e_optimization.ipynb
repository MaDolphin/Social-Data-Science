{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While for ordinary least squares regression we could directly compute parameters that yielded the best model, there is not such a closed-form solution anymore when we introduce a regularization term.\n",
    "In this exercise we take a look into the way that the ridge regression models, and actually most other regularized regression models are fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we use the Iris dataset to test our models on. The target variable is *petal width*, on we want to regress it from the three other numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dta = datasets.load_iris()\n",
    "# print description if necessary\n",
    "#print(dta.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Exploring the Loss Function\n",
    "\n",
    "\n",
    "In lecture we have learned that in *ridge regression*, we want to minimize the loss function \n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2.\n",
    "$$\n",
    "\n",
    "#### a) Implementing the Loss\n",
    "\n",
    "Implement a function that computes the value of the loss function, using the signature in the cell below. Assume that the first column of your input matrix $X$ has all-constant values $1$ to model the intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 2-dimensional numpy array of features \n",
    "# y: 1-dimensional array of target values\n",
    "# beta: current parameter vector\n",
    "# reg: regularization term lambda (float)\n",
    "\n",
    "def loss(X,y,beta,reg):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Convexity of the Loss\n",
    "\n",
    "We explore the loss function with a practical example. Load the Iris dataset from sklearn, and set up a univariate regression in which you predict petal width from petal length.\n",
    "Set  $\\beta_0 = mean(y)$ and $\\alpha = 1$, and plot the value of the loss function against $\\beta_1$ for $\\beta_1 \\in\\{-10,-9.9,\\dots,9.9,10\\}$.  \n",
    "Is there a unique minimum? What is the sign of the derivative at $\\beta = -3$ and $\\beta = 3$? In which direction does it point - in direction of the minimum or against it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Introducing Gradient Descent\n",
    "\n",
    "In practice, regularized regression models regression is usually optimized by a variant of gradient descent, which is a conpetually simple iterative procedure, in which you first initialize $\\beta$ at random, and then, until convergence, one updates\n",
    "$$\\beta^{new} \\gets \\beta^{old} - \\alpha\\cdot \\nabla L(\\beta^{old}), $$\n",
    "where $\\alpha$ is the so-called learning rate, and $\\nabla L(\\beta)$ the gradient of the loss function with respect to $\\beta$.\n",
    "In practice one usually selects a convergence tolerance $\\epsilon>0$ and stops the iteration when $|L(\\beta^{new}) - L(\\beta^{old})| < \\epsilon$.\n",
    "\n",
    "#### a) Computing the gradient\n",
    "Grab a pen and a piece of paper and compute the partial derivatives $$\\frac{\\partial C(\\beta)}{\\partial \\beta_k}$$ for $k=0$ and $k>0$. Judging the resulting derivatives, give a reason why it is not ideal when the features are not standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implementing Gradient Descent for Ridge Regression\n",
    "Implement a function that optimizes a ridge regression model via gradient descent, using the function signature in the cell below. Initialize your model in a way that $\\beta_0$ equals the mean of $y$ and all other elements of $\\beta$ are drawn from the standard normal distribution.\n",
    "\n",
    "Test your model on the **scaled** iris data to predict the petal width from all other columns, using $\\lambda = 1$, $\\alpha=0.001$, $\\epsilon=10^{-6}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES\n",
    "# X: 2-dimensional numpy array of features \n",
    "# y: 1-dimensional array of target values\n",
    "# reg: regularization term lambda (float)\n",
    "# alpha: learning rate (float)\n",
    "# eps: convergence treshold (float)\n",
    "# max_iter: maximum number of iterations in gradient descent (int) \n",
    "# -> break iteration when that number is reached even is we have not yet converged\n",
    "#\n",
    "# OUTPUT values:\n",
    "#\n",
    "# beta: numpy array containing optimal coefficients\n",
    "# n_it: number of iterations\n",
    "# losses: vector of loss function values in every iteration\n",
    "\n",
    "def ridge_GD(X,y,reg,alpha,eps,max_iter=1e04): \n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) The Effect of the Learning Rate\n",
    "\n",
    "Reoptimize your model using different learning rates $\\alpha \\in \\{10^{-i} : i\\in\\{1,2,3,4,5,6\\}\\}$.\n",
    "What are the effects on your optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Stochastic Gradient Descent \n",
    "\n",
    "Note that in practice, the datasets that are optimized on are often very large. When optimizing the model, this has the particular effect that the sum becomes very large and thus takes very long to be computed. one often uses more advanced techniques such as stochastic gradient descent (SGD) to accelerate the iteration.\n",
    "The idea behind SGD is to only consider a subset (batch) of samples which is randomly drawn from the full data. That way, we only have to sum over the small subset of samples, which significantly accelerates each iteration step. While this optimizes the loss function in a rather noisy way, and usually takes more single iteration steps, this approach still saves much computation time on large datasets.\n",
    "\n",
    "#### a) Implementing SGD\n",
    "Adapt your code from task 2 to implement stochastic gradient descent! Apply your model on the scaled iris data with batch size 10, and all other parameters as in 2a)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES\n",
    "# X: 2-dimensional numpy array of features \n",
    "# y: 1-dimensional array of target values\n",
    "# reg: regularization term lambda (float)\n",
    "# batch_size: (int)\n",
    "# alpha: learning rate (float)\n",
    "# eps: convergence treshold (float)\n",
    "# max_iter: maximum number of iterations in gradient descent (int) \n",
    "# -> break iteration when that number is reached even is we have not yet converged\n",
    "#\n",
    "# OUTPUT values:\n",
    "#\n",
    "# beta: numpy array containing optimal coefficients\n",
    "# n_it: number of iterations\n",
    "# losses: vector of loss function values in every iteration\n",
    "\n",
    "def ridge_SGD(X,y,reg,batch_size,alpha,eps,max_iter=1e05):\n",
    "\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Experimenting with Batch Sizes\n",
    "Reoptimize your model multiple times with fixed batch sizes in $\\{1,5, 10,20,30,50\\}$. What effects do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
