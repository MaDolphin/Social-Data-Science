{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Logistic Regression and Causal Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Logistic Regression\n",
    "\n",
    "Again, we revisit the Student Performance dataset. This time however, we do not focus on predicting test performance, but on predicting whether a student has taken the test preparation course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Fitting and model analysis\n",
    "\n",
    "Preprocess the data like in the previous exercise, i.e. transform categorical variables and remove highly correlated predictors. Then, use statsmodels to fit a logistic regression model that aims to predict the completion of a test preparation course model. Which predictors appear significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>completed</td>    <th>  R-squared:         </th> <td>   0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 07 Nov 2019</td> <th>  Prob (F-statistic):</th> <td>1.37e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:55:35</td>     <th>  Log-Likelihood:    </th> <td> -635.71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   1299.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   986</td>      <th>  BIC:               </th> <td>   1368.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>math score</th>         <td>   -0.0069</td> <td>    0.002</td> <td>   -2.850</td> <td> 0.004</td> <td>   -0.012</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reading score</th>      <td>    0.0156</td> <td>    0.002</td> <td>    6.420</td> <td> 0.000</td> <td>    0.011</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>female</th>             <td>   -0.1536</td> <td>    0.041</td> <td>   -3.758</td> <td> 0.000</td> <td>   -0.234</td> <td>   -0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group A</th>            <td>   -0.1203</td> <td>    0.061</td> <td>   -1.979</td> <td> 0.048</td> <td>   -0.240</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group B</th>            <td>   -0.1073</td> <td>    0.054</td> <td>   -1.999</td> <td> 0.046</td> <td>   -0.213</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group C</th>            <td>   -0.1109</td> <td>    0.051</td> <td>   -2.192</td> <td> 0.029</td> <td>   -0.210</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group D</th>            <td>   -0.1731</td> <td>    0.053</td> <td>   -3.244</td> <td> 0.001</td> <td>   -0.278</td> <td>   -0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group E</th>            <td>   -0.0517</td> <td>    0.063</td> <td>   -0.822</td> <td> 0.411</td> <td>   -0.175</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>associate's degree</th> <td>   -0.0941</td> <td>    0.048</td> <td>   -1.947</td> <td> 0.052</td> <td>   -0.189</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bachelor's degree</th>  <td>   -0.0928</td> <td>    0.057</td> <td>   -1.638</td> <td> 0.102</td> <td>   -0.204</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>high school</th>        <td>   -0.1238</td> <td>    0.045</td> <td>   -2.752</td> <td> 0.006</td> <td>   -0.212</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>master's degree</th>    <td>   -0.1582</td> <td>    0.070</td> <td>   -2.269</td> <td> 0.024</td> <td>   -0.295</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>some college</th>       <td>   -0.0994</td> <td>    0.047</td> <td>   -2.095</td> <td> 0.036</td> <td>   -0.193</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>some high school</th>   <td>    0.0052</td> <td>    0.047</td> <td>    0.110</td> <td> 0.912</td> <td>   -0.087</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>free/reduced</th>       <td>    0.0579</td> <td>    0.033</td> <td>    1.744</td> <td> 0.081</td> <td>   -0.007</td> <td>    0.123</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>9397.675</td> <th>  Durbin-Watson:     </th> <td>   1.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 120.860</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.518</td>  <th>  Prob(JB):          </th> <td>5.70e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 1.648</td>  <th>  Cond. No.          </th> <td>1.09e+18</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 8.02e-30. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              completed   R-squared:                       0.092\n",
       "Model:                            OLS   Adj. R-squared:                  0.080\n",
       "Method:                 Least Squares   F-statistic:                     7.649\n",
       "Date:                Thu, 07 Nov 2019   Prob (F-statistic):           1.37e-14\n",
       "Time:                        17:55:35   Log-Likelihood:                -635.71\n",
       "No. Observations:                1000   AIC:                             1299.\n",
       "Df Residuals:                     986   BIC:                             1368.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================\n",
       "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------\n",
       "math score            -0.0069      0.002     -2.850      0.004      -0.012      -0.002\n",
       "reading score          0.0156      0.002      6.420      0.000       0.011       0.020\n",
       "female                -0.1536      0.041     -3.758      0.000      -0.234      -0.073\n",
       "group A               -0.1203      0.061     -1.979      0.048      -0.240      -0.001\n",
       "group B               -0.1073      0.054     -1.999      0.046      -0.213      -0.002\n",
       "group C               -0.1109      0.051     -2.192      0.029      -0.210      -0.012\n",
       "group D               -0.1731      0.053     -3.244      0.001      -0.278      -0.068\n",
       "group E               -0.0517      0.063     -0.822      0.411      -0.175       0.072\n",
       "associate's degree    -0.0941      0.048     -1.947      0.052      -0.189       0.001\n",
       "bachelor's degree     -0.0928      0.057     -1.638      0.102      -0.204       0.018\n",
       "high school           -0.1238      0.045     -2.752      0.006      -0.212      -0.036\n",
       "master's degree       -0.1582      0.070     -2.269      0.024      -0.295      -0.021\n",
       "some college          -0.0994      0.047     -2.095      0.036      -0.193      -0.006\n",
       "some high school       0.0052      0.047      0.110      0.912      -0.087       0.098\n",
       "free/reduced           0.0579      0.033      1.744      0.081      -0.007       0.123\n",
       "==============================================================================\n",
       "Omnibus:                     9397.675   Durbin-Watson:                   1.986\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              120.860\n",
       "Skew:                           0.518   Prob(JB):                     5.70e-27\n",
       "Kurtosis:                       1.648   Cond. No.                     1.09e+18\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 8.02e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv (\"StudentsPerformance.csv\", delimiter=',')\n",
    "\n",
    "# print(df['race/ethnicity'].value_counts())\n",
    "\n",
    "df_gender = pd.get_dummies(df['gender'])\n",
    "df_race = pd.get_dummies(df['race/ethnicity'])\n",
    "df_edu = pd.get_dummies(df['parental level of education'])\n",
    "df_lunch = pd.get_dummies(df['lunch'])\n",
    "df_course = pd.get_dummies(df['test preparation course'])\n",
    "\n",
    "col = [df_gender,df_race,df_edu,df_lunch,df_course]\n",
    "for i in col:\n",
    "    df = df.join(i)\n",
    "\n",
    "# df = df.iloc[:, 5:]\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# print(np.tril(arr, k=-1))  # Lower triangle of an array.\n",
    "# print(np.triu(arr, k=1))  # Upper triangle of an array.\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# Drop features \n",
    "df = df.drop(df[to_drop], axis=1)\n",
    "\n",
    "# print(df.columns)\n",
    "columns = ['math score', 'reading score', 'female', 'group A', 'group B', 'group C', 'group D', 'group E', \"associate's degree\", \"bachelor's degree\", 'high school', \"master's degree\", 'some college', 'some high school', 'free/reduced']\n",
    "# columns = ['math score']\n",
    "\n",
    "X = df[columns]\n",
    "Y = df['completed']\n",
    "\n",
    "# initialize model: OLS = ordinary least squares\n",
    "model = sm.OLS(Y,X)\n",
    "# fit model: only now te model, i.e. the parameters are computed\n",
    "results = model.fit()\n",
    "\n",
    "# print a summary, i.e. an overview on parameters and diagnostics\n",
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Diagnostics 1: Accuracy and Confusion Matrix\n",
    "\n",
    "Write a two function that take as input a vector y of the true classes, and a vector y_hat of the predicted classes. Let the first one return the accuracy of the prediction, i.e. the ratio of correctly predicted samples, and the second one compute the confusion matrix as introduced in class.\n",
    "Apply your function on your model from a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Diagnostics 2: The ROC curve\n",
    "\n",
    "Write a function that takes as input a vector y of the true classes, and a vector yp of the predicted probabilities resulting from the logistic regression, plots ROC curve of the model, and returns the corresponding AUC score.\n",
    "Apply your function on your model from a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Causal Inference\n",
    "\n",
    "In this task we use a dataset (NSW.csv) which aimed to evaluate the effect of participating in a job training program on the salary. This data was taken from the website of Gelman and Hill's book (http://www.stat.columbia.edu/~gelman/arm/), and originally constructed in two independent studies (see Gelman and Hill, chapter 10, ex. 1).\n",
    "This data contains some demographic data of its population, the real earnings in 1974 and 1975, and indicator on whether job training, i.e., the treatment, was conducted in 1976/77, and the earnings in 1978, which is our target variable. A brief documentation can be found in \"NSW.doc\". Make sure that when loading the data, you omit the sample variable which simply indicates a source that a specific obervation originated from.  \n",
    "Note that there are only very few treated individuals in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may load and preprocess your data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Mean and Regression analysis with one predictor\n",
    "\n",
    "We first simply consider the treatment as a predictor for the earnings in 1978. \n",
    "Investigate the effect of the treatment by (i) computing the difference in means between control and treatment groups and (ii) performing a linear regression with only one predictor.\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Variable bias\n",
    "\n",
    "Intuitively, it makes sense that the income in 1975 has strong predictive power in the earnings 3 years later. Recompute your regression model such that it additionally includes that income. \n",
    "Further, compute the omitted variable bias between the treatment and the income from 75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Adding more predictors\n",
    "\n",
    "Our data provides a lot more potential predictors. Add some more predoctors to your regression model and observe the sensitivity of the model to new predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Greedy matching\n",
    "\n",
    "We now consider ALL columns in the data as predictors. Due to the numerical imbalance in the data, we have many samples that we would not want to include in our analysis with the treatment group.  \n",
    "Implement a function that is given as input two matrices corresponding to the confounders of controal and treatment group, and returns a matching of their indices based on Mahalanobis distance.\n",
    "Apply your function to compute a matching on the given data. Note that due to the strong imbalance between the cardinalities of the control group and the treatment group, you do not need to consider a maximum distance threshold in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Analyzing matched groups\n",
    "\n",
    "Recompute the means in control group and treated group, and the regression model that includes all columns as predictors. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
