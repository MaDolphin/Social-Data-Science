{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Assignment 3\n",
    "\n",
    "Submit your solution via Moodle until 23.59pm on Wednesday, December 18th. Late submissions are accepted for 12 hours following the deadline, with 1/3 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of up 2-3 members. **Single student submissions will not be graded anymore**.\n",
    "Please denote all members of the team with their student ID and full name in the notebook. In this home assignment, you have to submit an .ipynb notebook and a file \"labels.npy\" as specified in task 3. Do not submit anything else than these two files!\n",
    "\n",
    "Cite ALL your sources for coding this home assignment. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members will be expelled from the course without warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List team members, including all student IDs here:\n",
    "1. Student 1 (123456)\n",
    "2. Student 2 (123457)\n",
    "3. (optional) Student 3 (123458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Fair Classification with the Two Naive Bayes Algorithm (2 pts)\n",
    "\n",
    "Recall the _Ricci_ dataset that we have discussed in lecture, which shows the results of an exam for promotion eligibility. For the purpose of this exercise, we want to build a classifier which achieves that the group that gets a promoted (_Class_ attribute) is fairly balanced with respect to the sensitive attribute *Race* - as noted in lecture, enforcing this equality on that data has actually caused a \"reverse discrimination\" lawsuit. The majority class, i.e. the white population, has sensitive attribute value 1, and all non-whites have sensitive attribute value 0.\n",
    "\n",
    "The dataset is read in the cell below, and an implementation of the CV measure can also be found below that. Note that in the following tasks, you should make use of functions from scikit-learn whereever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oral</th>\n",
       "      <th>Written</th>\n",
       "      <th>Race</th>\n",
       "      <th>Combine</th>\n",
       "      <th>Class</th>\n",
       "      <th>Position_Captain</th>\n",
       "      <th>Position_Lieutenant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>89.52</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>92.808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>89.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>82.38</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>85.152</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>88.57</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>81.028</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>76.19</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>80.876</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Oral  Written  Race  Combine  Class  Position_Captain  Position_Lieutenant\n",
       "0  89.52       95     1   92.808      1                 1                    0\n",
       "1  80.00       95     1   89.000      1                 1                    0\n",
       "2  82.38       87     1   85.152      1                 1                    0\n",
       "3  88.57       76     1   81.028      1                 1                    0\n",
       "4  76.19       84     1   80.876      1                 1                    0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ricci.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_measure(y_pred, s_arr):\n",
    "    pos_mask = s_arr==1\n",
    "    neg_mask = s_arr==0\n",
    "\n",
    "    return 1.0 - np.sum(y_pred[pos_mask])/np.sum(pos_mask) + np.sum(y_pred[neg_mask])/np.sum(neg_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Biased Classification (0.5 pts)\n",
    "\n",
    "Train a Gaussian Naive Bayes classifier on the full dataset which learns to predict the _Class_ attribute using all other columns in the data. Apply your model on the training data, and compute the CV score of the resulting predictions with respect to the sensitive attribute _Race_. What does the result say on the disparity in the classifications of each majority and minority group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dataset without Scale: 0.4745762711864407\n",
      "Accuracy of the dataset with Scale: 0.8389830508474576\n",
      "CV-Score with original date is: 0.6970588235294117\n",
      "CV-Score from GNB classifier: 0.5341176470588235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = df.drop(columns = ['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "# pre-prossing of the dataset, scale all measures in [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# After scale the accuracy of dataset has been gradually improved from 0.47 to 0.91.\n",
    "print('Accuracy of the dataset without Scale:',clf1.score(X,y))\n",
    "print('Accuracy of the dataset with Scale:',clf1.score(X_scaled,y))\n",
    "\n",
    "# Train a Gaussian Naive Bayes classifier to calculate the CV-Socre\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(X_scaled, y)\n",
    "y_pred = clf1.predict(X_scaled)\n",
    "s_arr = X['Race'].to_numpy()\n",
    "\n",
    "print('CV-Score with original date is:',cv_measure(y,s_arr))\n",
    "print('CV-Score from GNB classifier:',cv_measure(y_pred,s_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination:</br>\n",
    "\n",
    "The CV-Score from original dataset is about 0.69. After training a Gaussian naive Bayes classiﬁer from data involving a sensitive feature, the CV score on the predicted classes decrease to about 0.53. That means the difference of  majority and minority group increase from 0.33 to 0.47.</br>\n",
    "\n",
    "This shows that the non-whites records are more frequently misclassiﬁed to the class 0 than white population records; and thus, the individuals in non-whites with class 1 are considered to be unfairly treated.</br>\n",
    "(class 0 is the people who are fail in the exam and class 1 is the people who are not fail in the exam)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Applying Two Naive Bayes Models (0.5 pts)\n",
    "\n",
    "Train two different Naive Bayes Models, one for each of the two race values. Again, apply these on the data samples that corresponds to the population they were trained on, and combine their predictions. Are the classifications among both population groups more balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV-Score with original date is: 0.6970588235294117\n",
      "The CV-Score after training is: 0.7811764705882354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the data set into two parts, one for the people with race value 0 and the other part with race value 1\n",
    "df_race_1 = df[df['Race'] == 1]\n",
    "df_race_0 = df[df['Race'] == 0]\n",
    "\n",
    "# train the model for df_Race_1 and \n",
    "\n",
    "X1 = df_race_1.drop(columns = ['Class'])\n",
    "y1 = df_race_1['Class']\n",
    "X2 = df_race_0.drop(columns = ['Class'])\n",
    "y2 = df_race_0['Class']\n",
    "\n",
    "# scale the measures of X1 except \"Race\"\n",
    "X1_no_Race = X1.drop(columns = ['Race'])\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X1_no_Race)\n",
    "X1_no_Race = scaler.transform(X1_no_Race)\n",
    "X1_scaled = np.insert(X1_no_Race, 2, values=1, axis=1)\n",
    "\n",
    "# scale the measures of X2 except \"Race\"\n",
    "X2_no_Race = X2.drop(columns = ['Race'])\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X2_no_Race)\n",
    "X2_no_Race =scaler.transform(X2_no_Race)\n",
    "X2_scaled = np.insert(X2_no_Race, 2, values=0, axis=1)\n",
    "\n",
    "\n",
    "# train the model for df_Race_1 and df_race_0\n",
    "clf1 = GaussianNB()\n",
    "clf2 = GaussianNB()\n",
    "clf1.fit(X1_scaled, y1)\n",
    "clf2.fit(X2_scaled, y2)\n",
    "\n",
    "y1_pred = clf1.predict(X1_scaled)\n",
    "y2_pred = clf2.predict(X2_scaled)\n",
    "y_pred = np.concatenate([y1_pred,y2_pred])\n",
    "y_origin = np.concatenate([y1,y2])\n",
    "X_2 = np.concatenate([X1_scaled, X2_scaled])\n",
    "s_arr = X_2[:,2]\n",
    "\n",
    "print('The CV-Score with original date is:',cv_measure(y_origin,s_arr))\n",
    "print('The CV-Score after training is:',cv_measure(y_pred,s_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination:\n",
    "Compare with the CV-Score between the dataset, which is before and after training. The CV-Socre isncreases from 0.69 to 0.78. It shows that the classifications among both population groups are more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) The Two Naive Bayes Algorithm by Calders and Verwer (1 pt)\n",
    "\n",
    "In lecture we have briefly touched the Two Naive Bayes algorithm by Calders and Verwer. Its main idea is to train two different Naive Bayes models corresponding to both values of a binary sensitive attribute $S$, and to then adapt the prior probabilities $P(Y|S)$ of each of these models until the predictions on the training data become fair.\n",
    "More precisely, in training, we first fit two different models, namely one for the privileged class $S=1$ and one for the non-privileged class $S=0$, to obtain the conditional probabilities $P(X_i|Y,S=s)$, $s=0,1$, and the priors $P(Y|S=s)$, $s=0,1$ from the training data. After these initial fits, we evaluate the model on the training data, and if we do not observe fair predictions, i.e. predictions for which the CV score is at least $1-\\epsilon$ for some $\\epsilon>0$, we adapt the priors $P(Y|S=s)$ until this is the case. This adaptation is carried out as follows:\n",
    "\n",
    "We initialize $N(Y=j, S=s)$ as the number of training samples with class $j$ and sensitive attribute value $s$, and $N(Y=1)$ the number of samples with class $1$ in the data. Further, let $\\Delta$ denote an adaptation rate, which is given as input.  \n",
    "Then we conduct the following iteration:\n",
    "\n",
    "     cv_score <- CV score of the predicted classes by the current model.\n",
    "     while cv_score < 1 - \\epsilon:\n",
    "        n_pos <- number of predicted positive samples from the current model.\n",
    "        if n_pos < N(Y=1):\n",
    "           N(Y=1, S=0) <- N(Y=1, S=0) + ΔN(Y=0, S=1)\n",
    "           N(Y=0, S=0) <- N(Y=0, S=0) - ΔN(Y=0, S=1) \n",
    "        else:\n",
    "           N(Y=0, S=1) <- N(Y=0, S=1) + ΔN(Y=1, S=0)\n",
    "           N(Y=1, S=1) <- N(Y=1, S=1) - ΔN(Y=1, S=0)\n",
    "        if any of the N(Y, S) is negative then cancel the previous update of N(Y, S) and abort\n",
    "        Recalculate priors P(Y|S=i)  and cv_score based on updated N(Y, S)\n",
    "    \n",
    "Note that the priors $P(Y|S=s)$ are calculated via\n",
    "$$P(Y|S=s) = \\frac{N(Y, S=s)}{N(S=s)},$$\n",
    "where $N(S=s)$ denotes the number of samples in the data with sensitive attribute value $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task description:** Implement a class that runs the Two Naive Bayes algorithm, using the signatures in the cell below! Apply Gaussian Naive Bayes models to fit your data. In particular, you do not need to differentiate between continuous and discrete data as in home assignment 2, and you can use the GaussianNB class from sklearn to fit the two individual models. Note that this class also has an attribute that stores the prior probabilities and can be overwritten easily.\n",
    "Further, you may assume that all input data is valid, i.e. you do not need to check whether for instance the feature matrix X has as many rows as the class vector y or the sensitive attribute vector s_arr.\n",
    "\n",
    "Apply your implementation on the Ricci dataset with default values for $\\epsilon$ and $\\Delta$, and compute the resulting CV score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-6-13096a935531>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-13096a935531>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    # your code here\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class TwoNaiveBayes:\n",
    "    \n",
    "    # delta: adaptation rate (float)\n",
    "    # eps: fairness tolerance, we consider a classification fair if the cv score on the training data is at least 1 - eps\n",
    "    def __init__(self, delta=0.01, eps = 0.05):\n",
    "        # your code here\n",
    "        self.delta = delta\n",
    "        self.esp = esp\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.N = np.zeros((2,2))\n",
    "        self.num_X = 0\n",
    "\n",
    "    # X: 2d numerical numpy array reprenting the feature matrix without the sensitive attribute\n",
    "    # s_arr: binary 1D numpy array representing the sensitive attribute\n",
    "    # y: binary 1D numpy array representing the class labels to train on\n",
    "    #\n",
    "    # nothing is returned, only internal parameters are being fit\n",
    "    def fit(self, X, s_arr, y):\n",
    "        #your code here\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.num_X = X.shape[0]\n",
    "        num_pos_current = np.sum(y == 1)\n",
    "        self.N = np.histogram2d(y, s, [2, 2], [[0, 2], [0, 2]])[0]\n",
    "        for i in range(2):\n",
    "            self.clr_[i].fit(X[s == i, :], y[s == i])\n",
    "        \n",
    "#         s_arr_new = np.insert(s_arr, 0, values=-1, axis=1)\n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "    # X: 2d numerical numpy array reprenting the feature matrix without the sensitive attribute\n",
    "    # s_arr: binary 1D numpy array representing the sensitive attribute to consider when predicting\n",
    "    # \n",
    "    # return binary numpy array vector of fair class labels\n",
    "    def predict(self,X,s_arr):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1,  2,  3],\n",
       "       [-1,  4,  5,  6],\n",
       "       [-1,  7,  8,  9]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "\n",
    "# print(np.delete(X, 1, axis=1))\n",
    "X_1 = np.insert(X, 0, values=-1, axis=1)\n",
    "X_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 2: k-Means Clustering (1.5 pts)\n",
    "\n",
    "We want to apply the $k$-means clustering algorithm from lecture to assign clusters to the dataset in \"clusters.txt\", which is loaded and plotted in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = np.loadtxt(\"clusters.txt\")\n",
    "plt.scatter(X[:,0],X[:,1], s=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Implementing k-means (0.75 pts)\n",
    "\n",
    "Implement the $k$-means clustering algorithm using the function specification in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES:\n",
    "# X: numerical 2D numpy array, each row is a sample\n",
    "# k: int, number of clusters, ignored if init_points is not None\n",
    "# init_points: list of row indices which indicate the data points that the clusters are initialized with\n",
    "# -> default is None, indicating that random data points are initialized as cluster centers\n",
    "# if specified, k is chosen as the number of cluster centers, and input for k is ignored\n",
    "#\n",
    "# RETURN:\n",
    "# y: vector with cluster labels in {1,...,k}\n",
    "\n",
    "\n",
    "def k_means(X, k=2, init_points = None):\n",
    "    ## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Application with random initialization (0.25 pts)\n",
    "\n",
    "Apply your implementation 10 times on the data from clusters.txt, choosing $k = 4$ and random initalization of cluster centers. For each run, scatterplot the data as above, with different colors indicating the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) k-means++ (0.5 pts)\n",
    "\n",
    "Recall the initialization of cluster centers from the $k$-means++ algorithm as presented in lecture:\n",
    "\n",
    "Let $X$ the set of data points, and $\\mathcal{D}(x)$ denote the shortest distance from a data point to the closest center we have already chosen. Then, the k-means++ initialization consists of the following steps.\n",
    "1. Take one center $c_1$, chosen uniformly at random from $X$.\n",
    "2. Take a new center $c_i$, choosing $x \\in X$ with probability $$p(x) = \\frac{D(x)^2}{\\sum_{x\\in X} D(x)^2}.$$\n",
    "3. Repeat Step 2 until we have taken $k$ centers altogether.\n",
    "\n",
    "Implement this initialization, using the signature in the cell below. Use Euclidean distance to measure distance between any pair of points in the data.  \n",
    "Apply this initialization to rerun the experiment from b). That is, run the $4$-means clustering algorithm ten times on the given data, using the initialization from k_means_pp, and scatterplot each clustering using different colors for different clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES:\n",
    "# X: numerical 2D numpy array, each row is a sample\n",
    "# k: int, number of clusters, ignored if init_points is not None\n",
    "#\n",
    "# RETURN:\n",
    "# init_points: list of row indices which indicate the data points that the clusters are initialized with\n",
    "\n",
    "def init_k_means_pp(X,k):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Helping Santa (1.5 pts)\n",
    "\n",
    "Santa Claus has gotten old and cannot deliver the christmas presents by himself anymore. Luckily, he has his team of 20 elves that are happy to carry all the presents out for him.\n",
    "To distribute presents in an efficient way, he wants to partition the households to deliver presents to in an optimal way, by clustering the locations that the presents are delivered to.\n",
    "A map with the coordinates of each household to deliver can be found in \"xmas.txt\", and it is loaded and plotted in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas = np.loadtxt(\"xmas.txt\")\n",
    "plt.scatter(xmas[:,0],xmas[:,1], s=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) The Silhouette Coefficient (0.5 pts)\n",
    "\n",
    "Since santa cannot judge the quality of a clustering by looking at a map, he needs to apply some quality measure that quantifies how good a clustering is.  \n",
    "\n",
    "One such measure, which both evaluates how compact clusters are among themselves, and how well different clusters are separated, is the **silhouette coefficient**.\n",
    "Assuming that all data points $x_1,\\dots ,x_n$ are partitioned into $k$ clusters $C_1, \\dots, C_k$, we can compute two coefficients for each data point $x_i$. On the one hand, assuming that $x_i \\in C_l$,\n",
    "$$\n",
    "a(x_i) := \\frac{\\sum_{x_j\\in C_l} d(x_i,x_j)}{|C_l| - 1}\n",
    "$$\n",
    "measures the compactness of the cluster $C_l$ to which $x_i$ belongs, and \n",
    "$$\n",
    "b(x_i) := \\min_{C_m\\not= C_l}\\left\\{ \\frac{\\sum_{x_j\\in C_m} d(x_i,x_j)}{|C_m|} \\right\\}\n",
    "$$\n",
    "captures the degree to which $x_i$ is separated from other clusters. Note that in these formulas, $d$ represents a distance function that can be chosen individually for the application at hand.\n",
    "From these two values, we can compute the silhouette coefficient of $x_i$ via\n",
    "$$\n",
    "s(x_i) = \\frac{b(x_i) - a(x_i)}{\\max\\{a(x_i),b(x_i)\\}}.\n",
    "$$\n",
    "Note that the value of this coefficient lies between -1 and 1, with 1 being the optimum, and negative values indicating a bad clustering, where on expectation $x_i$ is further away from the data points in its own cluster than from points in any other cluster.  \n",
    "To evaluate the quality of the **full clustering** of a dataset, one can then compute the **average silhouette coefficient of all points in a dataset**.  \n",
    "Note that this measure does not require a fixed $k$, unlike the objective function in the k-means clustering algorithm, and thus it can also be used to find an optimal $k$.\n",
    "\n",
    "Write a function that computes this silhouette score of a full clustering, using the _Euclidean distance_ as the distance function $d$. Use the signature which is specified in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VALUES\n",
    "# X: numerical 2D numpy array, each row is a data sample\n",
    "# y: numpy vector with corresponding cluster labels\n",
    "#\n",
    "# return value: single float\n",
    "\n",
    "def silhouette_score(X,y):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Finding the Optimal Hierarchical Clustering (1 pt)\n",
    "\n",
    "On christmas eve, Santa wants to send an appropriate number of his elves out to deliver all the presents, and he wants to send at least 10 elves out, because the limited capacity of their sleighs does not allow for a smaller number. However, he is also fine with not sending all of his 20 elves out, since all the elves that are not delivering presents could prepare Santa's own traditional christmas party in the meantime.  \n",
    "Thus, he decides to simply send out the number of elves between 10 and 20 that yields the best clustering with respect to the silhouette score. Since santa does not like the risks of random initializations, he wants to apply a hierarchical clustering algorithm for that task.\n",
    "\n",
    "Find the optimal clustering of the households on the christmas map consisting of between 10 and 20 clusters, resulting from a hierarchical (agglomerative) clustering with either single, average or complete linkage, and using Euclidean distance. Save the corresponding vector of optimal cluster labels into a file \"labels.npy\" and submit this file along with your noteboook. Also provide all code that you used to find this optimum in the cell(s) below.    \n",
    "Note that you do not need to implement hierarchical clustering yourself, but may resort to popular python libraries.\n",
    "Also note that this task is not a class competition, it is only about applying the clustering algorithms and measures correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
